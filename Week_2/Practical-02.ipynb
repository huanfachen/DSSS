{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "            <h1 style=\"width:450px\">CASA0006 Practical 2: Supervised learning</h1>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "\n",
    "In this workshop, we will achieve the following objectives:\n",
    "1. Review weekly quiz;\n",
    "1. Build a decision tree model to predict the daily bicycle rental based on metrics of R2 and RMSE;\n",
    "1. Tune the hyperparameter of maximal tree height of this model, using two ways of validation.\n",
    "\n",
    "Remember that *Practice makes perfect.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, some codes are missing and marked with ```??```.\n",
    "\n",
    "Please replace ```??``` with the correct codes, using the hints and context. The solution will be given in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder for \\*\\* Reviewing weekly quiz \\*\\*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the relevant libraries. \n",
    "\n",
    "* `pandas` for data import and handling;\n",
    "* `matplotlib`;\n",
    "* `numpy`;\n",
    "* `sklearn`;\n",
    "* `statsmodels` for linear regression and VIF.\n",
    "\n",
    "**Run the script below to get started.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "plt.style.use('ggplot') # specifies that graphs should use ggplot styling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will use relates to daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal here is to predict how many bikes will be rented depending on the weather and the day. The original data can be downloaded from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n",
    "\n",
    "The dataset used in this workshop has been slightly processed by Christoph Molnar using the processing R-script from this [Github repository](https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R). Here, the dataset is provided as a csv file on Moodle.\n",
    "\n",
    "Here is a list of the variables in the dataset:\n",
    "\n",
    "- Count of bicycles including both casual and registered users. The count is used as the response in the regression task.\n",
    "- Indicator of the season, either spring, summer, fall or winter.\n",
    "- Indicator whether the day was a holiday or not.\n",
    "- The year: either 2011 or 2012.\n",
    "- Number of days since the 01.01.2011 (the first day in the dataset). This predictor was introduced to take account of the trend over time.\n",
    "- Indicator whether the day was a working day or weekend.\n",
    "- The weather situation on that day. One of:\n",
    "  - **'GOOD'**: including clear, few clouds, partly cloudy, cloudy\n",
    "  - **'MISTY'**: including mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - **'RAIN/SNOW/STORM'**: including light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds, heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- Temperature in degrees Celsius.\n",
    "- Relative humidity in percent (0 to 100).\n",
    "- Wind speed in km/h.\n",
    "\n",
    "We will use Pandas package to load and explore this dataset:\n",
    "\n",
    "- Import the bicycle rental dataset as a Pandas dataframe (call it `bike_rental`)\n",
    "- Inspect the data\n",
    "- Calculate summary statistics on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental = pd.read_csv('https://raw.githubusercontent.com/huanfachen/Spatial_Data_Science/main/Dataset/daily_count_bike_rental.csv')\n",
    "# drop the year variable as it is not useful\n",
    "bike_rental = bike_rental.??(['yr'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few rows of this dataset\n",
    "bike_rental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `bike_rental`, there are two data types: categorical (aka `object`), and numerical (including `int64` and `float64`). \n",
    "\n",
    "Before undertaking regression, some dat aprocessing should be done, which include:\n",
    "- Converting categorical variables into dummy variables (aka one-hot encoding);\n",
    "- Split the data into training and testing sets;\n",
    "- For linear regression, we should deal with multicollinearity (and removing some variables if necessary)\n",
    "\n",
    "One note, the reason for doing one-hot encoding is that sklearn decision trees don't handle categorical data. However, other packages have more efficient ways of handling cateogical variables than one-hot encoding for tree-based models.\n",
    "\n",
    "We will discuss this issue in more depth next week. Currently, we stick to one-hot encoding.\n",
    "\n",
    "See [this Stackoverflow post](https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree) for related discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical variables\n",
    "First, we need to convert categorical variables into dummy/indicator variables, using `One-Hot Encoding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentail_numeric = pd.get_dummies(bike_rental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the new dataFrame\n",
    "bike_rentail_numeric.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, a cateogircal variable of K categories or levels, usually enters a regression as a sequence of K-1 dummy variables. The level that is left out becomes the reference level, and this is important for interpreting the regression model.\n",
    "\n",
    "Here we manually choose the reference level for each categorical variable and exclude them from the DataFrame. You can change the reference levels if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental_final = bike_rentail_numeric.drop(['season_SPRING', 'mnth_JAN', 'holiday_NO HOLIDAY', 'weekday_MON', 'workingday_WORKING DAY', 'weathersit_GOOD'], axis=1)\n",
    "\n",
    "# double check the result\n",
    "bike_rental_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into random train and test subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `train_test_split` will split the data according to a 75:25 split. Other proportions can be specified, check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details.\n",
    "\n",
    "Remember that the split should be random in order to avoid selection bias. Here, we set random_state=100 to guarantee reproducibility.\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "The first argument of this function:\n",
    "\n",
    "```\n",
    "*arrays: sequence of indexables with same length / shape[0]\n",
    "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "```\n",
    "\n",
    "The output of this function:\n",
    "```\n",
    "splitting: list, length=2 * len(arrays)\n",
    "List containing train-test split of inputs.\n",
    "```\n",
    "\n",
    "Here we input two dataframes (X and Y) and will get four outputs (train_x, test_x, train_y, test_y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_split = 100\n",
    "train_x, test_x, train_y, test_y = train_test_split(bike_rental_final.drop(['cnt'], axis = 1), ??.cnt, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the rows and columns of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "\n",
    "# check the index of train_x and train_y - they should be identical. The index indicates which rows from the original data.\n",
    "\n",
    "print(train_x.index.identical(train_y.index))\n",
    "print(test_x.index.identical(test_y.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression models, we will use the training set to train the model and select hyperparameters. The testing set is only used to report the performance of the finalised model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a regression tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a regression tree for this dataset.\n",
    "\n",
    "While there are several hyperparameters in this model such as **max_depth** (referring to the height of a tree), in the first attempt we just use the default settings.\n",
    "\n",
    "One question: what is the default value or setting of **max_depth**? Can you get the answer from the documentation?\n",
    "\n",
    "```\n",
    "max_depthint, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg_tree = DecisionTreeRegressor(random_state=0)\n",
    "# if you don't remember the meaning of random_state, check out the lastest clustering workshop on Moodle\n",
    "reg_tree.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(reg_tree.score(X=train_x, y=train_y))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(reg_tree.score(X=??, y=??))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the R2 on the testing data is much lower than that on the training data. This indicates the overfitting problem, meaning that the model fits very well to the training data but doesn't generalise well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer RMSE as the performance metric, you can calculate it as follows (see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE on the training data:\")\n",
    "print(mean_squared_error(train_y, reg_tree.predict(train_x), squared=False))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(mean_squared_error(test_y, reg_tree.predict(test_x), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters of the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, hyperparameters are the model settings predefined by the user, e.g. maximum height of the decision tree.\n",
    "\n",
    "We will demonstrate how to tune the hyperparameters, using the maximum height of a decision tree (which is called `max_depth` for `DecisionTreeRegressor` in sklearn) as an example.\n",
    "\n",
    "Some intuition about how the maximum tree height affects the performance of decision tree: the larger the maximum tree height, the better fitness of the model and thus the higher possibility of overfitting.\n",
    "\n",
    "So, ideally, we want the maximum tree height to be neither too small or too large. But, what is the optimal value? This will be answered by hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see if we can tune the model hyperparameters by maximising model performance on the development dataset. There are two ways of hyperparameter tuning:\n",
    "\n",
    "1. Holdout validation (by splitting the training data into two subsets - training and development data)\n",
    "2. Cross validation\n",
    "\n",
    "The pros and cons of these methods are as follows:\n",
    "\n",
    "| Method      | Pros |Cons |\n",
    "| ----------- | ----------- |----------- |\n",
    "| Holdout Validation  | Computationally efficient, well-suited for large datasets |Subject to split of training and validation sets, results are less accurate |\n",
    "| Cross Validation | Results are more accurate |Computationally inefficient for big datasets, sometimes infeasible |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will demonstrate both ways using the **GridSearchCV** function from sklearn. The documentation of this function is [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout validation\n",
    "\n",
    "The documentation of **GridSearchCV** indicates different ways of specifying the **cv** parameter:\n",
    "```\n",
    "cv: int, cross-validation generator or an iterable, default=None\n",
    "Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "\n",
    "- None, to use the default 5-fold cross validation,\n",
    "\n",
    "- integer, to specify the number of folds in a (Stratified)KFold,\n",
    "\n",
    "- CV splitter,\n",
    "\n",
    "- An iterable yielding (train, test) splits as arrays of indices.\n",
    "```\n",
    "\n",
    "To use holdout validation, we can set the cv as a list of (train, test) that indicates the indices of training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index list of training and validation data in the merged dataset\n",
    "# we will use the first 75% rows of [train_x, train_y] as training set and the following 25% rows as validation set\n",
    "# index of the first 75% rows (rounded to integer)\n",
    "n_row_training = np.floor(train_x.shape[0]*0.75).astype(int)\n",
    "ind_train = list(range(n_row_training))\n",
    "# index of the remaining rows\n",
    "ind_val = list(range(n_row_training, train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of max_depth\n",
    "hyperparameters = {'max_depth':[10,20,30,40,50]}\n",
    "\n",
    "randomState_dt = 10000\n",
    "dt = DecisionTreeRegressor(random_state=randomState_dt)\n",
    "\n",
    "# CV: An iterable yielding (train, test) splits as arrays of indices.\n",
    "clf = GridSearchCV(dt, hyperparameters, cv=[(ind_train, ind_val)])\n",
    "\n",
    "clf.fit(train_x, train_y)\n",
    "\n",
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, print out the optimal number of trees. Note that if you change the random_state in the ```RandomForestRegressor```, you may get a different ```best_params_``` but the ```best_score_``` is quite similar across different runs. This is because the randomisation in creating ```RandomForestRegressor```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create the final random forest model using the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final = DecisionTreeRegressor(max_depth=clf.best_params_['max_depth'], random_state=randomState_dt)\n",
    "dt_final.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters, we are able to evaluate the performance of the model to inspect bias and variance.\n",
    "\n",
    "Note that the score on teh validation data is the same as the ```clf.best_score_```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The score on the training data:')\n",
    "print(dt_final.score(train_x, train_y))\n",
    "print('The score on the testing data:')\n",
    "print(dt_final.??(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "Cross validation is preferred over the holdout validation method in real-world machine learning projects, as the results are more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of max_depth\n",
    "hyperparameters = {'max_depth':[10,20,30,40,50]}\n",
    "\n",
    "randomState_dt = 10000\n",
    "dt = DecisionTreeRegressor(random_state=randomState_dt)\n",
    "\n",
    "# cv=5 by default, which means 5-fold cross-validation\n",
    "clf = GridSearchCV(dt, hyperparameters)\n",
    "\n",
    "clf.fit(??, ??)\n",
    "\n",
    "# we can query the best parameter value and its accuracy score\n",
    "print (\"The best parameter value is: \")\n",
    "print (clf.best_params_)\n",
    "print (\"The best score is: \")\n",
    "print (clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it is helpful to plot the influence of a single hyperparameter on the training and development score to find out whether the model is overfitting or underfitting. This plot is called **Validation curve**.\n",
    "\n",
    "The function ```validation_score``` is helpful in this case. Similar to ```GridSearchCV```, this function is based on cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_range = [10,20,30,40,50]\n",
    "\n",
    "train_scores, valid_scores = validation_curve(estimator=DecisionTreeRegressor(), \n",
    "                                              X=train_x, y=train_y, \n",
    "                                              param_name=\"max_depth\",\n",
    "                                              param_range=max_depth_range,\n",
    "                                              cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the validation curve\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "valid_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with RF\")\n",
    "plt.xlabel(r\"maximum tree depth\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.ylim(0.6, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(max_depth_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(max_depth_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(max_depth_range, valid_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(max_depth_range, valid_scores_mean - valid_scores_std,\n",
    "                 valid_scores_mean + valid_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the training and development scores do not change significantly with max depth.\n",
    "\n",
    "You can pick up the point where the gap between training and cross-validation score reaches the minimum as the selected hyperparameter.\n",
    "\n",
    "In fact, in GridSearch, we can tune the combination of multiple hyperparameters. We will discuss this issue in later weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to hold-out validation, we can create the final random forest model using the optimal hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_final = DecisionTreeRegressor(max_depth=clf.??['max_depth'], random_state=randomState_dt)\n",
    "dt_final.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning hyperparameters, we can evaluate the performance of on the testing data, which is the performance of \\*\\*decision tree algorithm\\*\\* on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The score on the training data:')\n",
    "print(dt_final.score(train_x, train_y))\n",
    "print('The score on the testing data:')\n",
    "print(dt_final.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are limitations in this work - we could have tuned more hyperparameters, with a wider range of potential values. However, we have demonstrated a complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this workshop, we have covered the analysis workflow of supervised learning, using an example of building regression trees to predict daily bicycle rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and recommendations:\n",
    "\n",
    "1. This notebook is partially based on a [chapter](https://christophm.github.io/interpretable-ml-book/limo.html) of the [book](https://christophm.github.io/interpretable-ml-book/) by Christoph Molnar. \n",
    "1. This notebook is partially based on [this notebook](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.03-Hyperparameters-and-Model-Validation.ipynb), which is part of the [online repo](https://jakevdp.github.io/PythonDataScienceHandbook/) of the book [\"Python Data Science Handbook\"](https://www.oreilly.com/library/view/python-data-science/9781491912126/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
