{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huanfachen/DSSS/blob/main/Week_6/Practical_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umCFkGTqxMge"
      },
      "source": [
        "<div style=\"float:left\">\n",
        "    <h1 style=\"width:600px\">CASA0006 Practical 6: Deep learning applications</h1>\n",
        "</div>\n",
        "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04QgGZc9bF5D"
      },
      "source": [
        "## Introduction\n",
        "In this practical, we will use TensorFlow and [YOLOv5] to:\n",
        "\n",
        "1. Build a convolutional neural network (CNN) for Fashion-MNIST object identification;\n",
        "2. Conduct object detection in satellite imagery, using pre-trained deep learning models YOLOv5.\n",
        "\n",
        "## Setting up Google Colab\n",
        "\n",
        "As installing and configuring tensorflow and YOLO on local machines can be a pain, we recommend using Google Colab for this practical. Click [here](https://colab.research.google.com/github/huanfachen/DSSS/blob/main/Week_6/Practical_06.ipynb) to run this practical on Google Colab, which requires a Google account.\n",
        "\n",
        "Resource limit of Google Colab under free plan:\n",
        "\n",
        "- Memory: up to 12 GB.\n",
        "- Maximum duration of running a notebook: notebooks can run for at most **12 hours**, depending on availability and your usage patterns. (The notebook will die after at most 12 hours)\n",
        "- GPU duration: dynamic, up to a few hours. If you use GPU regularly, runtime durations will become shorter and shorter and disconnections more frequent.\n",
        "\n",
        "*Very Important* - we will use the GPU on Google Colab to accelerate the model training. To do this, go to 'Runtime' -> 'Change runtime type' -> Select 'T4 GPU' -> Save. See below.\n",
        "\n",
        "![](https://github.com/huanfachen/DSSS/blob/main/Figures/Colab_GPU_setting.jpg?raw=true)\n",
        "\n",
        "If you are following along in your own development environment, rather than Colab, see the [install guide](https://www.tensorflow.org/install) for setting up TensorFlow for development.\n",
        "\n",
        "Note: Make sure you have upgraded to the latest `pip` to install the TensorFlow 2 package if you are using your own development environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Overview\n",
        "\n",
        "![CNN](https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/cnn.png)\n",
        "\n",
        "Convolutional neural networks (CNN) is a special type of neural network for image or image-like features. \n",
        "\n",
        "CNN is a complex subject and we could do a 10-week module on it. Here, we aim to cover the basics of CNN with loads of illustrations and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# to make this notebook's output reproducible across runs\n",
        "def reset_state(seed=42):\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12\n",
        "\n",
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image.astype(np.uint8),interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "    \n",
        "def crop(images):\n",
        "    return images[150:220, 130:250]\n",
        "    \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Layers in NN \n",
        "\n",
        "* Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s.\n",
        "* Used in image recognition also in voice recognition and natural language processing \n",
        "* Local Field of View\n",
        "* Used to define local features in an image - data compression\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/localFov.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why CNN?\n",
        "\n",
        "* A way of encoding images using a small(er) number of features upon which training (e.g. for classifying images) can take place. \n",
        "* Differences in images that enable humans to classify are hypothesise to be extracted in similar ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Layers \n",
        "\n",
        "* Encode convolutions as a NN \n",
        "\n",
        "* Neurons connected to _receptor_ field in next layer that is _smaller_. Uses zero padding to force layers to have same height & width.\n",
        "\n",
        "* Also can connect large input layer to much smaller layer by spacing out receptor fields (distance between receptor fields is called the _stride_)\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/CC.jpeg\" alt=\"Drawing\" style=\"width: 700px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Maps \n",
        "\n",
        "* Neuron weights can look like small images (w/ size = receptor field)\n",
        "* Examples below: 1) vertical filter (single vertical bar, mid-image, all other cells zero) 2) horizontal filter (single horizontal bar, mid-image, all other cells zero)\n",
        "* Both return _feature maps_ (highlights areas of image most similar to filter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/featuremap.jpeg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacking Feature Maps \n",
        "\n",
        "* More realistic to have several features of similar size i.e. 3D layers \n",
        "* A convolutional layer can thereby apply multiple filters to its input and be capable of detecting multiple features \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/stacked.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example \n",
        "\n",
        "* The following code loads two sample images, using Scikit-Learn’s load_sample_images() (which loads two color images, one of a Chinese temple, and the other of a flower)\n",
        "* Then it creates two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle), and applies them to both images using a convolutional layer built using TensorFlow’s ${\\tt tf.nn.conv2d()}$ function (with zero padding and a stride of 2). \n",
        "* Finally, it plots one of the resulting feature maps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#simple example \n",
        "from sklearn.datasets import load_sample_image #load images\n",
        "\n",
        "# Load sample images\n",
        "china = load_sample_image(\"china.jpg\") / 255\n",
        "flower = load_sample_image(\"flower.jpg\") / 255\n",
        "images = np.array([china, flower])\n",
        "batch_size, height, width, channels = images.shape\n",
        "\n",
        "# Create 2 filters that are 7x7xchannelsx2 arrays\n",
        "filters = np.zeros(shape=(7, 7, channels, 2),  dtype=np.float32)\n",
        "filters[:, 3, :, 0] = 1  # vertical line\n",
        "filters[3, :, :, 1] = 1  # horizontal line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Conv2D arguments:\n",
        "# filters = 4D tensor\n",
        "# strides = 1D array (1, vstride, hstride, 1)\n",
        "# padding = VALID = no zero padding, may ignore edge rows/cols\n",
        "# padding = SAME  = zero padding used if needed\n",
        "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1,\n",
        "                           padding=\"SAME\", activation=\"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = tf.nn.conv2d(images, filters, strides=8, padding=\"SAME\")\n",
        "\n",
        "plt.imshow(outputs[0, :, :, 0], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
        "\n",
        "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
        "\n",
        "plt.imshow(outputs[1, :, :, 0], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
        "\n",
        "plt.imshow(outputs[1, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/dotproduct-slides.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n",
        "Note the relation to the dot product "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/example-slides.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/6filter-slides.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n",
        "The convolution layer comprises of a set of independent filters (6 in the example shown). Each filter is independently convolved with the image and we end up with 6 feature maps of shape 28x28x1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/simpleseries-slides.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n",
        "All these filters are initialized randomly and become our parameters which will be learned by the network subsequently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/padding.jpeg\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pooling Layers \n",
        "\n",
        "* Pooling layers aim to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting). Pooling layer operates on each feature map independently.\n",
        "\n",
        "* Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. \n",
        "\n",
        "* However, a pooling neuron has no weights; all it does is aggregate the inputs using an aggregation function such as the max or mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/pool.jpeg\" alt=\"Drawing\" style=\"width: 900px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/maxpool-slides.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Architectures \n",
        "\n",
        "* Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer; max(0,x), then a pooling layer, then another few convolutional layers (+ ReLU), then another pooling layer, and so on.\n",
        "\n",
        "* The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps) thanks to the convolutional layers\n",
        "\n",
        "* At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers, and the final layer outputs the prediction \n",
        "\n",
        "* There are many successful and popular CNN structures, including LeNet, AlexNet, ResNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/aCNN.jpeg\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://raw.githubusercontent.com/astro-informatics/course_mlbd_images/master/Lecture19_Images/excnn.jpeg\" alt=\"Drawing\" style=\"width: 1500px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LeNet\n",
        "\n",
        "LeNet is a series of CNN structure proposed by LeCun et al., and have been widely used for mnist. The first LeNet, LeNet-1, was trained in 1989.  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/lenet.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AlexNet\n",
        "\n",
        "AlexNet is another CNN structure, designed by *Alex Krizhevsky* in collaboration with *Ilya Sutskever* and *Geoffrey Hinton*. \n",
        "\n",
        "AlexNet was submitted in ImageNet Challenge [http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/). It won the champion and achived a top-5 error of 15.3%, much better than the runner-up.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/alexnet.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet-34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Residual neural network (also referred to as a residual network or ResNet) is a CNN architecture in which the layers learn residual functions with reference to the layer inputs and some connections skip two or more layers.\n",
        "\n",
        "![ResBlock](https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/ResBlock.jpg)\n",
        "\n",
        "It was first proposed by [Kaiming He](https://people.csail.mit.edu/kaiming/), see [this ground-breaking paper](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html), with 260766 citations on Google Scholar.\n",
        "\n",
        "It was developed in 2015 for image recognition, and won the ImageNet Large Scale Visual Recognition Challenge of that year.\n",
        "\n",
        "The residual connection stablises the training and convergence of CNN with hundreds of layers, and has become a common motif in later CNNs, such as BERT and GPT models.\n",
        "\n",
        "ResNet-34 is one type of ResNet and has 34 convolutional layers. It is pretrained on the ImageNet dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial  \n",
        "# What is partial? See https://chriskiehl.com/article/Cleaner-coding-through-partially-applied-functions\n",
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"SAME\", use_bias=False)\n",
        "\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                keras.layers.BatchNormalization()]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
        "                        input_shape=[28, 28, 1]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ResNet-34 for tackling Fashion-MNIST task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will try ResNet-34 for [the Fashion-MNIST task](https://keras.io/api/datasets/fashion_mnist/). We don't want to use the classic MNIST, as MNIST is now considered too easy for modern deep learning models.\n",
        "\n",
        "Fashion-MNIST is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. \n",
        "\n",
        "The 10 classies are:\n",
        "\n",
        "| Label | Description   |\n",
        "|-------|---------------|\n",
        "| 0     | T-shirt/top   |\n",
        "| 1     | Trouser       |\n",
        "| 2     | Pullover      |\n",
        "| 3     | Dress         |\n",
        "| 4     | Coat          |\n",
        "| 5     | Sandal        |\n",
        "| 6     | Shirt         |\n",
        "| 7     | Sneaker       |\n",
        "| 8     | Bag           |\n",
        "| 9     | Ankle boot    |\n",
        "\n",
        "![Fashion-MNIST](https://raw.githubusercontent.com/huanfachen/DSSS/main/Figures/fashion-mnist-sprite.jpg)\n",
        "\n",
        "More info: https://github.com/zalandoresearch/fashion-mnist/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import Fashion-MNIST data\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beware - the cell below could take a long time to run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=5, validation_data=(X_valid, ??))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "score = model.evaluate(??, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict 5 images from test set\n",
        "n_images = 5\n",
        "test_images = X_test[:n_images]\n",
        "predictions = model.predict(??)\n",
        "\n",
        "# Display image and model prediction.\n",
        "for i in range(n_images):\n",
        "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
        "    plt.show()\n",
        "    print(\"Image label: %i\" % y_test[i])\n",
        "    print(\"Model prediction: %i\" % np.argmax(predictions[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgyfwE6rxMgp"
      },
      "source": [
        "## YOLOv5 for Object Detection in Satellite Imagery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3937J4DCxMgp"
      },
      "source": [
        "Having explored CNN and ResNet, we are now ready to apply this to a more explicitly spatial use case: object detection in satellite imagery.\n",
        "\n",
        "Read the [following guide](https://bellingcat.github.io/RS4OSINT/C5_Object_Detection.html), and then complete the rest of this workbook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZG2rFV0r_IW"
      },
      "source": [
        "### What is YOLOv5\n",
        "Object detection is a fairly complicated task, and there are a number of different approaches to it. In this tutorial, we’ll be using a model called YOLOv5. YOLO stands for **You Only Look Once**, and it’s a model that was developed by [Joseph Redmon et al.](https://pjreddie.com/), and the full paper detailing the model can be found [here](https://arxiv.org/abs/1506.02640).\n",
        "\n",
        "The YOLOv5 model is a convolutional neural network (CNN), which is a type of deep learning model. CNNs are very good at identifying patterns in images, particularly in small regions of images. This is important for object detection, because we want to be able to identify objects even if they’re partially obscured by other objects.\n",
        "\n",
        "YOLO works by chopping an image up into a grid, and then predicting the location and size of objects in each grid cell:\n",
        "\n",
        "![](https://bellingcat.github.io/RS4OSINT/images/yolo.jpg)\n",
        "\n",
        "It learns the locations of these objects by training on a dataset of images in which each object is indicated by a bounding box. Then, when it’s shown a new image, it will attempt to predict bounding boxes around the objects in that image. The standard YOLO model is trained on the COCO dataset, which contains over 200,000 images of 80 different objects ranging from people to cars to dogs. YOLO models pre-trained on this dataset work great out of the box to detect objects in videos, photographs, and live streams. But the nature of the objects we’re interested in is a bit different.\n",
        "\n",
        "Luckily, we can simply **re-train** the YOLOv5 model on datasets of labeled satellite imagery. We will walk through the process of re-training YOLOv5 on a custom dataset, and then using it to identify objects in satellite imagery pulled from Google Earth Engine or Google Map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQnHfbibxMgp",
        "outputId": "bc9650b9-8b84-464f-c4b6-a89f244f40cf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/huanfachen/yolov5_RS  # clone repo\n",
        "#%cd yolov5_RS\n",
        "%pip install -qr yolov5_RS/requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzDPQ0a5tH3O"
      },
      "source": [
        "We will use public satellite imagery dataset from Roboflow, specifically, [this link](https://universe.roboflow.com/gdit/aerial-airport/dataset/1).\n",
        "\n",
        "What is Roboflow? It is a Computer Vision developer framework for better data collection to preprocessing and model training techniques. Roboflow contains public datasets readily available to users and also has access for users to upload their own custom data.\n",
        "\n",
        "You can explore a gallery of public datasets on [Roboflow Universe](https://universe.roboflow.com/). These datasets are frequently updated.\n",
        "\n",
        "Note that you need a Roboflow API KEY to access the datasets. To get your own API KEY, visit [this link](https://docs.roboflow.com/api-reference/authentication).\n",
        "\n",
        "Below is my API KEY in the free tier, which is subject to certain use limit. You can use it for tests, but it might be slow or out of limit. It is always a good idea to get your own API KEY and get control of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j0uXPmRxMgq",
        "outputId": "c95208dd-52f7-4a20-9c96-0791b1c27b27"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "API_KEY = 'aywzIBJkeuu2TcHztYSq'\n",
        "rf = Roboflow(api_key=API_KEY)\n",
        "project = rf.workspace(\"gdit\").project(\"aerial-airport\")\n",
        "dataset = project.version(1).download(\"yolov5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVgC13r5oadx"
      },
      "source": [
        "Check the datasets have been downloaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCGWR9fIo8hp",
        "outputId": "eb41f224-a1f7-4af6-a2a1-7e4010eb150c"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "!ls yolov5_RS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvEI6t3Eo-mw"
      },
      "source": [
        "The next step is to copy the data folder *Aerial-Airport-1* into the folder of yolov5_RS, as the YOLOv5 Python codes require the datasets are in the same folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGOx45PmnSPJ"
      },
      "outputs": [],
      "source": [
        "!cp -r Aerial-Airport-1 yolov5_RS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAj8kQIumsMs",
        "outputId": "9bd1c1e9-2d49-45f5-9f64-6eddb3cec21e"
      },
      "outputs": [],
      "source": [
        "!ls\n",
        "!ls yolov5_RS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWni7q2qeb0I"
      },
      "source": [
        "We will train the model via running the *train.py* file.\n",
        "\n",
        "The settings, including batch size, is important. The general rule is: the larger batch size, the larger memory required. The batch size is set as 16 here after trial and error. If we set **--batch 32**, Colab might run out of memory and force the training to stop (*setting a '^C' in the proccess*). Note that the memory of Google Colab is default at 12 GB of RAM.\n",
        "\n",
        "Other factors influencing the required memory of YOLO include image size, batch size, and model size.\n",
        "\n",
        "See [Discussion on Github](https://github.com/ultralytics/yolov5/issues/3847) or [Stackoverflow](https://stackoverflow.com/a/63797661/4667568).\n",
        "\n",
        "The following model training takes quite a long time, around 3 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoTLsny1nDpM",
        "outputId": "3d8a6241-5996-40a3-91d1-33ad47eb30fd"
      },
      "outputs": [],
      "source": [
        "!less yolov5_RS/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz7Whm_DxMgq",
        "outputId": "0d8f4c35-89df-4911-a952-1438a5b8b202"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!python yolov5_RS/train.py --data {dataset.location}/data.yaml --img 320 --batch 16 --cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_-DBHbklYJt"
      },
      "source": [
        "Then, we will apply the YOLOv5 model to detect objects of a remote sensing image from Google Map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1giCgkg5xMgq",
        "outputId": "2cd85fb1-8617-441e-f0a6-c98af0f90669"
      },
      "outputs": [],
      "source": [
        "img='gatwick.jpg'\n",
        "!python yolov5_RS/detect.py --weights yolov5_RS/weights/general.pt --img 2000 --conf 0.4 --source {os.path.join('yolov5_RS',img)} --line-thickness 2 --exist-ok #--hide-labels --exist-ok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRaf4pZeqGd4"
      },
      "source": [
        "The results of detected objects are saved in the runs/detect/exp folder, as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGc2YdKbls07",
        "outputId": "63e998b3-61d4-4b4f-ef70-6316bb6f4894"
      },
      "outputs": [],
      "source": [
        "!ls yolov5_RS/runs/detect/exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBJmBErRl4BL"
      },
      "outputs": [],
      "source": [
        "img='gatwick.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "CjcaScAJxMgq",
        "outputId": "06737492-7897-4767-b5ec-8b7670eaa4d5"
      },
      "outputs": [],
      "source": [
        "out_dir='yolov5_RS/runs/detect/exp'\n",
        "Image(filename=os.path.join(out_dir,img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References and recommendations:\n",
        "\n",
        "1. Some materials are from Machine Learning with Big Data (SPCE0038) module at UCL."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "name": "MachineLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "d0d2e6730a6c54b05bb0156bc757d1580bef729e038830e8c9d99016e96c4534"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
