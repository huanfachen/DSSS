{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:600px\">CASA0006 Practical 4: Analysis workflow</h1>\n",
    "    <h3 style=\"width:600px\">CASA0006: Data Science for Spatial Systems</h3>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last executed: \" + now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome!\n",
    "\n",
    "In this workshop, we will achieve the following objectives:\n",
    "1. Review weekly quiz;\n",
    "1. Build a sklearn **pipeline** to predict the daily bicycle rentals using four different methods, including linear regression, CART, random forest, and XGBoost;\n",
    "1. Diagnose the four models by comparing the training and testing error;\n",
    "1. Compute the bias and variance of the machine learning models using the **mlxtend** library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, some codes are missing and marked with ```??```.\n",
    "\n",
    "Please replace ```??``` with the correct codes, using the hints and context. The solution will be given in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*\\* Review the weekly quiz \\*\\*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the relevant libraries. \n",
    "\n",
    "* `pandas` for data import and handling;\n",
    "* `matplotlib`;\n",
    "* `numpy`;\n",
    "* `sklearn`;\n",
    "* `xgboost` for XGBoost models.\n",
    "\n",
    "**Run the script below to get started.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "# preprocessors\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# CART\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# xgboost\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\n",
    "plt.style.use('ggplot') # specifies that graphs should use ggplot styling\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the library version before we start\n",
    "print(\"xgboost version:{}\".format(xgboost.__version__))\n",
    "print(\"sklearn version:{}\".format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same bicyle rental data as Week 3 workshop. \n",
    "\n",
    "The dataset relates to daily counts of rented bicycles from the bicycle rental company Capital-Bikeshare in Washington D.C., along with weather and seasonal information. The goal here is to predict how many bikes will be rented depending on the weather and the day. \n",
    "\n",
    "The original data can be downloaded from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n",
    "\n",
    "The dataset used in this workshop has been slightly processed by Christoph Molnar using the processing R-script from this [Github repository](https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R). Here, the dataset is provided as a csv file on Moodle.\n",
    "\n",
    "Here is a list of the variables in the dataset:\n",
    "\n",
    "- Count of bicycles including both casual and registered users. The count is used as the response in the regression task.\n",
    "- Indicator of the season, either spring, summer, fall or winter.\n",
    "- Indicator whether the day was a holiday or not.\n",
    "- The year: either 2011 or 2012.\n",
    "- Number of days since the 01.01.2011 (the first day in the dataset). This predictor was introduced to take account of the trend over time.\n",
    "- Indicator whether the day was a working day or weekend.\n",
    "- The weather situation on that day. One of:\n",
    "  - **'GOOD'**: including clear, few clouds, partly cloudy, cloudy\n",
    "  - **'MISTY'**: including mist + clouds, mist + broken clouds, mist + few clouds, mist\n",
    "  - **'RAIN/SNOW/STORM'**: including light snow, light rain + thunderstorm + scattered clouds, light rain + scattered clouds, heavy rain + ice pallets + thunderstorm + mist, snow + mist\n",
    "- Temperature in degrees Celsius.\n",
    "- Relative humidity in percent (0 to 100).\n",
    "- Wind speed in km/h.\n",
    "\n",
    "We will use Pandas package to load and explore this dataset:\n",
    "\n",
    "- Import the Boston housing dataset as a Pandas dataframe (call it `bike_rental`)\n",
    "- Inspect the data\n",
    "- Calculate summary statistics on all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental = pd.read_csv('https://raw.githubusercontent.com/huanfachen/Spatial_Data_Science/main/Dataset/daily_count_bike_rental.csv')\n",
    "# drop the year variable as it is not useful\n",
    "bike_rental = bike_rental.drop(['yr'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental.??()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `bike_rental`, there are two data types: categorical (aka `object`), and numerical (including `int64` and `float64`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a few rows of this dataset\n",
    "bike_rental.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there missing value in each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rental.??().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow of the week-3 workshop is as follows:\n",
    "\n",
    "1. One-hot encoding of the categorical variables, including seasons, month, holiday, weekday, workingday, weathersit;\n",
    "1. Split the data into training and testing subsets;\n",
    "1. Train the model on the training subset (including hyperparameter tuning);\n",
    "1. Report the model performance using the testing subsets;\n",
    "1. Repeat the above two steps and compare the performance of different models.\n",
    "\n",
    "In week-3 workshop, we conducted the above steps separately and step-by-step. This approach has two potential problems:\n",
    "\n",
    "1. The readers may find it difficult to get the full picture of analysis;\n",
    "2. It may lead to data leakage issues.\n",
    "\n",
    "Here, we will use a similar workflow but using a **pipeline** to consolidate these steps.\n",
    "\n",
    "In the folllowing, we are going to:\n",
    "\n",
    "1. Split the data into training and testing subsets (note: we still need to manually split the training and testing subsets before using a pipeline);\n",
    "1. Build a pipeline that consists of one-hot encoding, model training, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state_split = 100\n",
    "train_x, test_x, train_y, test_y = train_test_split(bike_rental.drop(['cnt'], axis = 1), bike_rental.cnt, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a **pipeline** in sklearn? The purpose of the pipeline is to assemble several steps of data processing and model training while setting different parameters.\n",
    "\n",
    "From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), the main parameter of a pipeline is 'steps':\n",
    "\n",
    "```\n",
    "steps: list of tuple\n",
    "List of (name, transform) tuples (implementing fit/transform) that are chained in sequential order. The last transform must be an estimator.\n",
    "```\n",
    "\n",
    "Let's make it easier: a pipeline should look like this:\n",
    "\n",
    "```\n",
    "Pipeline(steps=[('name_of_preprocessor', preprocessor),\n",
    "                ('name_of_ml_model', ml_model())])\n",
    "```\n",
    "\n",
    "Or, like this graph:\n",
    "\n",
    "![](https://github.com/huanfachen/DSSS/blob/main/Figures/pipeline.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The preprocessor of a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A preprocessor of a pipeline are a list of tranforming or data processing steps.\n",
    "\n",
    "Firstly, we need to define the transformers for both numeric and categorical features. \n",
    "\n",
    "A transforming step is represented by a tuple. In that tuple, you first define the name of the transformer, and then the function you want to apply. The order of the tuple will be the order that the pipeline applies the transforms. \n",
    "\n",
    "Here, we first deal with missing values, then standardise numeric features and one-hot encode categorical features.\n",
    "\n",
    "*Why dealing with missing values?* Although there are no missing values in this dataset, we demonstrate this transformer, as we are likely to deal with missing values in other tasks.\n",
    "\n",
    "*Why standardising numeric features?* While some algorithms don't require data standardisation (e.g. linear regression, decision trees), standardising numeric features doesn't degrade the model performance and other algorithms prefer standardised input data, such as neural networks.\n",
    "\n",
    "*Why one-hot encoding categorical features?* This is becuase many ML algorithms can't deal with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The missing values of a numeric feature will be replaced by the 'mean' value of the feature.\n",
    "# The missing values of a categorical feature will be replaced by the a 'constant' value. If 'constant' value is not specified, it is default at '0' for numerical data or 'np.nan' for categorical data.\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='mean'))\n",
    "      ,('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('encoder', OneHotEncoder(drop='first'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify which columns are numeric and which are categorical, so the pipeline can apply the transformers accordingly.\n",
    "\n",
    " We apply the transformers to features by using **ColumnTransformer**.\n",
    "\n",
    " From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), a ColumnTransformer applies transformers to columns of an array or pandas DataFrame.\n",
    "\n",
    "The main parameter is *transformers*:\n",
    "\n",
    "```\n",
    "transformers: list of tuples\n",
    "List of (name, transformer, columns) tuples specifying the transformer objects to be applied to subsets of the data.\n",
    "```\n",
    " \n",
    " Similar to pipeline, we pass a list of tuples, which is composed of (‘name’, ‘transformer’, ‘features’), to the parameter ‘transformers’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['temp', 'hum', 'windspeed', 'days_since_2011']\n",
    "categorical_features = ['season', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit']\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "    ('numeric', numeric_transformer, numeric_features)\n",
    "   ,('categorical', ??, ??)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an estimator to a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the preprocessor assembled, we can add in the estimator, which is any machine learning algorithm we would like to apply, to complete our preprocessing and training pipeline. \n",
    "\n",
    "As this is a regression task, we will start with a decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "   ('preprocessor', preprocessor),\n",
    "   ('regressor',DecisionTreeRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_model = pipeline.fit(??, ??)\n",
    "# this will visualise the pipeline\n",
    "print(cart_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can evaluate the model using testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE on the training data:\")\n",
    "print(root_mean_squared_error(train_y, cart_model.predict(train_x)))\n",
    "print(\"RMSE on the testing data:\")\n",
    "print(root_mean_squared_error(test_y, cart_model.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R2 on the training data:\")\n",
    "print(r2_score(train_y, cart_model.predict(train_x)))\n",
    "print(\"R2 on the testing data:\")\n",
    "print(r2_score(test_y, ??.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipeline for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can be used for hyperparameter tuning. We will demonstrate how to find the optimal max_depth and min_samples_split using pipeline and GridSearchCV.\n",
    "\n",
    "From the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) of GridSearchCV, the first parameter *estimator* is:\n",
    "\n",
    "```\n",
    "estimator: estimator object\n",
    "This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed.\n",
    "```\n",
    "\n",
    "As a pipeline is similar to a model instance and is an estimator, we can combine pipeline with GridSearchCV for tuning hyperparameters.\n",
    "\n",
    "One trick here: when specifying the grid_params, the name of each hyperparameter should be in the format of ```[estimator]__[hyperparameter]```, where the first part of ```[estimator]``` is the name of estimator in the Pipeline object, the second part is **two underscores**, and the third part is the hyperparameter in the model.\n",
    "\n",
    "For example: ```regressor__max_depth```.\n",
    "\n",
    "See more details [here](https://stackoverflow.com/a/41899244/4667468)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fix the random_state in DecisionTreeRegressor() so that the result of GridSearchCV is the same in different runs\n",
    "cart_pipeline = Pipeline(steps = [\n",
    "  ('preprocessor', preprocessor),\n",
    "  ('regressor', DecisionTreeRegressor(random_state=123))\n",
    "])\n",
    "\n",
    "cart_pipeline.fit(train_x, train_y)\n",
    "\n",
    "# grid_params is the range of each hyperparameter\n",
    "grid_params = {\n",
    "  'regressor__max_depth': [10,20,30,40,40], \n",
    "  '??min_samples_split': [2,4,6,8,10]\n",
    "}\n",
    "search = GridSearchCV(cart_pipeline, grid_params)\n",
    "search.fit(train_x, train_y)\n",
    "print(\"Best R2 Score: \", search.best_score_)\n",
    "print(\"Best Params: \", search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing several estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn pipelines make the workflows smoother and more flexible. With the pipeline, we can easily compare the performance of a number of algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'CART': DecisionTreeRegressor(),\n",
    "    'RF': ??(),\n",
    "    'XGB': ??()\n",
    "}\n",
    "\n",
    "# a dict to store the R2 of training and testing data\n",
    "dict_results = dict()\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    pipeline = Pipeline(steps = [\n",
    "               ('preprocessor', preprocessor)\n",
    "              ,('regressor', regressor)\n",
    "           ])\n",
    "    model = pipeline.fit(train_x, train_y)\n",
    "    predictions = model.predict(test_x)\n",
    "    dict_results[name] = [model.score(train_x, train_y), model.score(test_x, test_y), model.score(??, ??) - model.score(??, ??)]\n",
    "\n",
    "# transform dict_models to dataframe\n",
    "df_models = pd.DataFrame.from_dict(dict_results, orient='index', columns=['R2_train_data', 'R2_test_data', 'R2_diff'])\n",
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the four models using their R2 on the training and testing data. \n",
    "\n",
    "Note the different direction of R2 and prediction errors (e.g. RMSE): a large R2 means a small predictive error.\n",
    "\n",
    "Regarding the training R2, both CART and XGBoost have the largest R2, followed by random forest and then linear model. For the different between training R2 and testing R2, the linear model has the least value, while CART has the largest difference between training and testing R2.\n",
    "\n",
    "Some initial conclusions on the model performance:\n",
    "\n",
    "1. The linear model is underfitting, as it has a relatively low R2 on the training data.\n",
    "2. The CART may be subject to overfitting, as it has a relatively low R2 on the testing data. \n",
    "3. Compared with the CART, the random forest and XGBoost model are less subject to overfitting, as theire R2 score on the testing data is higher than CART. These two models have better performance than linear model or CART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the model bias and variance [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will demonstrate how to compute the bias and variance of a ML model. \n",
    "\n",
    "This library has been developed by [Sebastian Raschka](https://sebastianraschka.com/) and it provides a function named bias_variance_decomp() that help us to estimate the bias vs variance for various models over many bootstrap samples. \n",
    "\n",
    "The basic idea of bias_variance_decomp() is that: given the training and testing data and the predictive algorithm, in each round, it get a bootstrap sample of the training data, which is used to train a model, and then the model is applied to the testing data, which leads to predictions on the testing data. \n",
    "\n",
    "As the sampled training data is different in each round, the predictions on the testing data differ in each round. We can combine the multilple-round predictions (on the testing data) to estimate the bias and variance of the predictive algorithm.\n",
    "\n",
    "- The bias is the difference between the average prediction across multiple rounds and the actual values (of the testing data)\n",
    "\n",
    "- The variance is the variance of predictions over multiple rounds (of the testing data)\n",
    "\n",
    "Please note - the bias and variance of the algorithm is evaluated using the multiple-round prediction errors on the testing data, rather than the training data. The training data is only used to train the model, instead of evaluating the model.\n",
    "\n",
    "You can read the documentation [here](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/) and code [here](https://github.com/rasbt/mlxtend/blob/master/mlxtend/evaluate/bias_variance_decomp.py).\n",
    "\n",
    "To do this, we first need to install a new library called 'mlxtend'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlxtend==0.21.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the bias_variance_decomp function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bias_variance_decomp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will calculate the bias and variance of the CART.\n",
    "\n",
    "Note that here is a conflict between sklearn pipeline and bias_variance_decomp: a pipeline requires a Pandas DataFrame as input, while bias_variance_decomp requires a numpy array.\n",
    "\n",
    "In order to use bias_variance_decomp(), we have to go back to the approach in week-3 workshop: \n",
    "\n",
    "1. one-hot encoding the categorical variables;\n",
    "1. do the train-test split;\n",
    "1. transform training and testing data into numpy arrays, using the DataFrame.to_numpy() method, see [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy)\n",
    "1. create and train a DecisionTreeRegressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "bike_rental_numeric = pd.get_dummies(bike_rental)\n",
    "bike_rental_final = bike_rental_numeric.drop(['season_SPRING', 'mnth_JAN', 'holiday_NO HOLIDAY', 'weekday_MON', 'workingday_WORKING DAY', 'weathersit_GOOD'], axis=??)\n",
    "\n",
    "# train-test split\n",
    "random_state_split = 100\n",
    "train_x, test_x, train_y, test_y = train_test_split(bike_rental_final.drop(['cnt'], axis = 1), bike_rental_final.cnt, random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several key parameters of this function:\n",
    "\n",
    "```\n",
    "- loss : str (default='0-1_loss'). Loss function for performing the bias-variance decomposition. Currently allowed values are '0-1_loss' (for classification tasks) and 'mse' (Mean Square Error, for regression tasks).\n",
    "- num_rounds : int (default=200). Number of bootstrap rounds (sampling from the training set) for performing the bias-variance decomposition. Each bootstrap sample has the same size as the original training set.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train a DecisionTreeRegressor model using numpy array datasets. compute the total error, bias, and variance of this model\n",
    "avg_expected_loss, avg_bias_squared, avg_var = bias_variance_decomp(\n",
    "        DecisionTreeRegressor(random_state=0), train_x.to_numpy(), train_y.to_numpy(), test_x.to_numpy(), test_y.to_numpy(), \n",
    "        loss='mse',\n",
    "        random_seed=123)\n",
    "\n",
    "print('Average expected loss: %.3f' % ??)\n",
    "print('Average bias squared: %.3f' % ??)\n",
    "print('Average variance: %.3f' % ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that expected loss is the sum of bias and variance. \n",
    "\n",
    "*Note:* In the [documentation of bias_variance_decomp](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/), there are some typos. It should be “bias squared” rather than “Bias”, as **Loss = bias^2 + variance**. This has been confirmed by the authors in [this issue](https://github.com/rasbt/mlxtend/issues/1083).\n",
    "\n",
    "Now, we can compute and compare the bias and variance of the four models. \n",
    "\n",
    "To reduce the computing time, we set the parameter of num_rounds to 40 (default=200).\n",
    "\n",
    "The following code takes 2 minutes on my desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1233\n",
    "regressors = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'CART': DecisionTreeRegressor(random_state = random_seed),\n",
    "    'RF': RandomForestRegressor(random_state = random_seed),\n",
    "    'XGB': XGBRegressor(random_state = random_seed)\n",
    "}\n",
    "\n",
    "# a dict to store the R2 of training and testing data\n",
    "dict_results = dict()\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    avg_expected_loss, avg_bias_squared, avg_var = bias_variance_decomp(\n",
    "        regressor, train_x.to_numpy(), train_y.to_numpy(), test_x.to_numpy(), test_y.to_numpy(), \n",
    "        loss='mse',\n",
    "        random_seed=123,\n",
    "        num_rounds=40)\n",
    "    dict_results[name] = [avg_expected_loss, avg_bias_squared, avg_var]\n",
    "\n",
    "# transform dict_models to dataframe\n",
    "df_models = pd.DataFrame.from_dict(dict_results, orient='index', columns=['Total loss', 'Bias^2', 'Variance'])\n",
    "df_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result shows that the linear model has the highest bias and lowest variance. Clearly, it is under-fitting.\n",
    "\n",
    "Compared with CART, RF substantially decreases the variance and slightly decreases the bias, while XGBoost substantially decreases both bias and variance. Therefore, the RF and XGBoost model have the best performance for predicting the daily bicycle rental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this workshop, we have created and used the machine learning pipelines for predicting the daily bicycle rental.\n",
    "\n",
    "We conducted model diagnosis of the four methods via compared the predictive accuracy on the training and testing data.\n",
    "\n",
    "As an optional task, we computed the bias and variance of these four methods using the mlxtend library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and recommendations:\n",
    "\n",
    "1. [Step by Step Tutorial of Sci-kit Learn Pipeline](https://towardsdatascience.com/step-by-step-tutorial-of-sci-kit-learn-pipeline-62402d4629b6): this post demonstrates the sklearn pipeline using a very similar dataset, but I disagree with using OrdinalEncoder for categorical variables.\n",
    "2. [Tutorials on the bias_variance_decomp() function](http://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/): this is a handy tutorial from the author of the mlxtend library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0146c347c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
